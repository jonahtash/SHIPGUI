from io import StringIO
import unicodedata
import ctypes
import requests
import sqlite3
import json
import re
from http.cookiejar import CookieJar
from bs4 import BeautifulSoup
import pandas as pd
import os
from multiprocessing import Pool as PoolMP
from multiprocessing.dummy import Pool
from multiprocessing import freeze_support
from datetime import datetime
import subprocess
import csv


#computes the ratio of uppercase letters to all characters
#used to sort the parsed JSON sections 
def _upper_ratio(s):
    if len(s)==0:
        return 0
    u = len(re.findall('[A-Z]',s))
    l = len(re.findall('[a-z]',s))
    if u+l==0:
        return 0
    return u/(u+l)
#computes the ratio of numerical characters to characters in a section of parsed PDF
#used to sort sections that are not needed
def _digit_ratio(s):
    if len(s)==0:
        return 0
    d = len(re.findall('[0-9]',s))
    c = len(re.findall('[A-z]',s))
    if c+d==0:
        return 0
    return d/(c+d)


#computes the ratio of special characters to characters in a section of parsed PDF
def _special_ratio(s):
    if len(s)==0:
        return 0
    c = len(re.findall('[A-z]',s))
    s = len(re.findall('[\\.\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\-\\=]',s))
    if c+s==0:
        return 0
    return s/(c+s)


# takes folder of jsons generated by ScienceParse
# puts jsons into csv formatted file for bkup
# makes specfied modifcation to the json data and outputs to data csv
def run_json_folder(json_path,exclude_path,char_path,bkup_csv_path,csv_out_path):
	json_path = _clean_path(json_path)

	#init sqlite db in memory
	#sql db structure:
	##Table: temp_table
	###Columns: sec_head, text, id, sec_num, inferred, split_num upper_to_lower, digit_to_char, special_to_char
	conn = sqlite3.connect(':memory:')
	cur = conn.cursor()
	cur.execute('CREATE TABLE temp_table (sec_head varchar(255), text TEXT, id INT, sec_num INT, split_num INT, inferred BIT, upper_to_lower FLOAT, digit_to_char FLOAT, special_to_char FLOAT);')

	#go through json files and add their data to table
	for file_path in os.listdir(json_path):
		file_path=json_path+file_path
		print(file_path)
		f = json.load(open(file_path,'r',encoding='utf-8'))
		c=1
		#add title and author entry to table
		if f['metadata']['title']:
			cur.execute("INSERT INTO temp_table VALUES (?, ?, ?, ?, 1, 0, 0, 0, 0)", ('title', _clean_sql(f['metadata']['title']),file_path.split('/')[-1].split('.pdf.json')[0],c))
			c+= 1
		if f['metadata']['authors']:
			cur.execute("INSERT INTO temp_table VALUES (?, ?, ?, ?, 1, 0, 0, 0, 0)", ('authors', _clean_sql(str(f['metadata']['authors'])),file_path.split('/')[-1].split('.pdf.json')[0],c))
			c+= 1

		if f['metadata']['sections']:
			for sec in f['metadata']['sections']:
				text_clean = _clean_sql(sec['text']).replace("N IH -PA Author M anuscript\n",'').replace("N IH -PA Author M anuscript",'')
				heading_clean = ""
				if sec['heading']:
					heading_clean = _clean_sql(sec['heading'])
				else:
					heading_clean = "null"
				cmd = "INSERT INTO temp_table VALUES (?, ?, ?, ?, ?, ?, ?, ? ,? )"
				cur.execute(cmd, (heading_clean, text_clean,file_path.split('/')[-1].split('.pdf.json')[0],c,1,0,_upper_ratio(text_clean),_digit_ratio(text_clean),_special_ratio(text_clean)))
				c+= 1


	
	#Dump unedited table into backup csv
	pd.read_sql(sql='SELECT * FROM temp_table ORDER BY id,sec_num,split_num', con=conn).to_csv(bkup_csv_path, index=False,sep = ',',quoting=csv.QUOTE_NONNUMERIC)

	#read unwanted headers from csv file
	re = csv.reader(open(exclude_path,'r',encoding='utf-8'))
	for row in re:
		#delete where section header like word in csv
		cur.execute("DELETE from temp_table WHERE sec_head like '%"+row[0]+"%';")
	#remove all sections where the section text is less than certain length-- currently: 600 chars
	#added catch for authors and title since they are most likely too short but still should be included
	cur.execute("DELETE from temp_table WHERE length(text) < 600 AND sec_head!='title' AND sec_head !='authors';")

	#defragment indexing
	cur.execute("SELECT * FROM temp_table ORDER BY id,sec_num,split_num")
	rows = cur.fetchall()
	cur_id = rows[0][2]
	c =1
	for row in rows:
		if not row[2] == cur_id:
			cur_id = row[2]
			c = 1
		cur.execute("UPDATE temp_table SET sec_num = ? WHERE sec_head = ? AND id = ?", (c, row[0], cur_id))
		c+=1
	
	#go through and split up entries where the section text is longer than split_on characters
	#select all entries with sec. text longer than split_on characters
	split_on = "3000"
	cur.execute('SELECT * FROM temp_table WHERE length(text) > '+split_on+' ORDER BY id,sec_num,split_num;')

	for row in cur.fetchall():
		#split the sec. text of entry into split_on character chunks and interate
		bs = ""
		c_split = 1
		for s in [e+". " for e in row[1].split(". ") if e]:
			#insert chunk of text into table
			if len(bs + s) > int(split_on):  
				cur.execute("INSERT INTO temp_table VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",(row[0],bs,str(row[2]),row[3],c_split,row[5],_upper_ratio(bs),_digit_ratio(bs),_special_ratio(bs)))
				c_split+= 1
				bs = s
			else:
				bs+=s
		#remove old entry
		cur.execute("DELETE from temp_table WHERE sec_head like ? AND id=? AND length(text)>"+split_on+";",(row[0],str(row[2]))) 
		
	##try to fix null headers
	#get all rows with sec_head as null
	cur.execute("SELECT * FROM temp_table WHERE sec_head like 'null'")
	rows = cur.fetchall()
	#set variables with wider scope that names can persist
	cur_head = 'null'
	cur_id = rows[0][2]
	inf = 0
	for row in rows:
		#if new doc reset
		if cur_id != row[2]:
			cur_id = row[2]
			cur_head = 'null'
			inf=0
		text = row[1].strip()
		#if the first word is all caps it's probably a title
		#check if all caps
		
		if (' ' in text and text[:text.index(' ')].upper().strip() == text[:text.index(' ')].strip() and text[:text.index(' ')].isalpha()):
			a=text.split(' ')
			c=0
			bs = ""
			#read words until words are no longer all caps
			while(c < len(a) and a[c]==a[c].upper()):
				bs += a[c]+" "
				c+=1
			#set to be header
			if len(bs[:-1])>1:
                            cur_head = bs[:-1]
                            inf = 1
		else:
			#open csv with characters that denote heading
			#for each character see if that character comes before the end of the first sentence
			for line in csv.reader(open(char_path, 'r')):
				try:
					i = text.index(line[0].strip())
					#check to see if text in this format
					#Subheading in line: It is important that the character after the colon(or any char in csv) is captial. More text...
					if(i<text.index('.') and text[i+1:i+3].strip().isupper()):
						cur_head = text[:i]
						inf = 1 
				except:
					#idk python
					s='||-//'		
		#update entry in table with new sec_head and if that sec_head was inferred (i.e changed by this code block)
		cur.execute("UPDATE temp_table SET sec_head = ?, inferred = ? WHERE sec_num = ? AND id = ? AND split_num = ?", (cur_head, inf, row[3], row[2], row[4]))

	#delete entries based on text statistics
	#the both constants are for when entries have both text stats greater than a given value
	both_upper = .1
	both_digit = .1
	#for when just upper_to_lower is greater than certain ratio
	uppper = .15
	#for when just digit_to_char is greater than certain ratio
	digit = .12
	cur.execute("DELETE from temp_table WHERE (upper_to_lower > ? AND digit_to_char > ?) OR upper_to_lower > ? OR digit_to_char > ?;",(both_upper,both_digit,uppper,digit)) 

	#get the remaining entries in the table and write them to csv
	pd.read_sql(sql='SELECT * FROM temp_table ORDER BY id,sec_num,split_num', con=conn).to_csv(csv_out_path, index=False,sep = ',',quoting=csv.QUOTE_NONNUMERIC) 


def partition_jsons(json_dir,partition_dir,json_per_folder):
    files = [os.path.join(json_dir, f) for f in os.listdir(json_dir)]
    i = 0
    curr_subdir = None
    name = ""
    if len(json_dir.split("/")[-1])>1:
        name = json_dir.split("/")[-1]
    else:
        name=json_dir.split("/")[-2]
    
    for f in files:
        # create new subdir if necessary
        if i % json_per_folder == 0:
            subdir_name = os.path.join(partition_dir, name+'{0:04d}'.format(int(i / json_per_folder + 1)))
            if not os.path.exists(subdir_name):
                os.mkdir(subdir_name)
            curr_subdir = subdir_name

        # move file to current dir
        f_base = os.path.basename(f)
        shutil.copy(f, os.path.join(subdir_name, f_base))
        i += 1

def run_partition_folders(part_folder_dir,out_csv_dir,exclude_path,char_path,limit=-1):
    c = 0
    out_csv_dir = _clean_path(out_csv_dir)
    for f in os.listdir(part_folder_dir):
        folder = os.path.join(part_folder_dir, f)
        print(folder)
        run_json_folder(folder,exclude_path,char_path,out_csv_dir+f+"bkup.csv",out_csv_dir+f+"data.csv")
        if limit>0 and c>=limit:
            break


